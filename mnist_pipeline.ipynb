{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dace9ed9-84ea-43e6-8203-9129d136f595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_IMAGE = \"tensorflow/tensorflow:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3de6bd2-789f-4ccf-bf15-ee01dc4a95de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.dsl import Input, Output\n",
    "from kfp.dsl import Dataset, Artifact\n",
    "from kfp.dsl import Model, Metrics, ClassificationMetrics\n",
    "\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "841d752a-a222-460e-a76c-86233f0a129d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_148/3632099911.py:1: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  @dsl.component(\n"
     ]
    }
   ],
   "source": [
    "@dsl.component(\n",
    "    base_image=BASE_IMAGE,\n",
    ")\n",
    "def load_data(\n",
    "    x_train_pickle: Output[Dataset],\n",
    "    y_train_pickle: Output[Dataset],\n",
    "    x_test_pickle: Output[Dataset],\n",
    "    y_test_pickle: Output[Dataset],\n",
    "):\n",
    "    # import dataset\n",
    "    from keras.datasets import mnist\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "\n",
    "    # load dataset\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    # count the number of unique train labels\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    print(\"Train labels: \", dict(zip(unique, counts)))\n",
    "\n",
    "    # count the number of unique test labels\n",
    "    unique, counts = np.unique(y_test, return_counts=True)\n",
    "    print(\"\\nTest labels: \", dict(zip(unique, counts)))\n",
    "    indexes = np.random.randint(0, x_train.shape[0], size=25)\n",
    "    images = x_train[indexes]\n",
    "    with open(x_train_pickle.path, \"wb\") as file:\n",
    "        pickle.dump(x_train, file)\n",
    "\n",
    "    with open(y_train_pickle.path, \"wb\") as file:\n",
    "        pickle.dump(y_train, file)\n",
    "\n",
    "    with open(x_test_pickle.path, \"wb\") as file:\n",
    "        pickle.dump(x_test, file)\n",
    "\n",
    "    with open(y_test_pickle.path, \"wb\") as file:\n",
    "        pickle.dump(y_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ef6a86a-cb8d-41e5-83c6-7e89c05a8f87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_148/3069889619.py:1: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  @dsl.component(\n"
     ]
    }
   ],
   "source": [
    "@dsl.component(base_image=BASE_IMAGE)\n",
    "def preprocess_data(\n",
    "    x_train_pickle: Input[Dataset],\n",
    "    y_train_pickle: Input[Dataset],\n",
    "    x_test_pickle: Input[Dataset],\n",
    "    y_test_pickle: Input[Dataset],\n",
    "    x_train_prep: Output[Dataset],\n",
    "    y_train_prep: Output[Dataset],\n",
    "    x_test_prep: Output[Dataset],\n",
    "    y_test_prep: Output[Dataset],\n",
    ") -> NamedTuple(\"outputs\", input_size=int, num_labels=int):\n",
    "    from keras.utils import to_categorical\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    from typing import NamedTuple\n",
    "\n",
    "    with open(x_train_pickle.path, \"rb\") as file:\n",
    "        x_train = pickle.load(file)\n",
    "\n",
    "    with open(y_train_pickle.path, \"rb\") as file:\n",
    "        y_train = pickle.load(file)\n",
    "\n",
    "    with open(x_test_pickle.path, \"rb\") as file:\n",
    "        x_test = pickle.load(file)\n",
    "\n",
    "    with open(y_test_pickle.path, \"rb\") as file:\n",
    "        y_test = pickle.load(file)\n",
    "\n",
    "    num_labels = len(np.unique(y_train))\n",
    "\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    image_size = x_train.shape[1]\n",
    "    input_size = image_size * image_size\n",
    "    # resize and normalize\n",
    "    x_train = np.reshape(x_train, [-1, input_size])\n",
    "    x_train = x_train.astype(\"float32\") / 255\n",
    "    x_test = np.reshape(x_test, [-1, input_size])\n",
    "    x_test = x_test.astype(\"float32\") / 255\n",
    "    with open(x_train_prep.path, \"wb\") as file:\n",
    "        pickle.dump(x_train, file)\n",
    "\n",
    "    with open(y_train_prep.path, \"wb\") as file:\n",
    "        pickle.dump(y_train, file)\n",
    "\n",
    "    with open(x_test_prep.path, \"wb\") as file:\n",
    "        pickle.dump(x_test, file)\n",
    "\n",
    "    with open(y_test_prep.path, \"wb\") as file:\n",
    "        pickle.dump(y_test, file)\n",
    "    outputs = NamedTuple(\"outputs\", input_size=int, num_labels=int)\n",
    "    return outputs(input_size, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cd53b86-c71a-4308-93fc-08966d34b96b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_148/2658360530.py:1: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  @dsl.component(\n"
     ]
    }
   ],
   "source": [
    "@dsl.component(base_image=BASE_IMAGE)\n",
    "def train(\n",
    "    input_size: int,\n",
    "    num_labels: int,\n",
    "    epochs: int,\n",
    "    x_train_pickle: Input[Dataset],\n",
    "    y_train_pickle: Input[Dataset],\n",
    "    model_artifact: Output[Model],\n",
    "    log: Output[Artifact],\n",
    "):\n",
    "    from keras.callbacks import TensorBoard\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation, Dropout\n",
    "    import pickle\n",
    "    from datetime import datetime\n",
    "\n",
    "    batch_size = 128\n",
    "    hidden_units = 256\n",
    "    dropout = 0.45\n",
    "\n",
    "    with open(x_train_pickle.path, \"rb\") as file:\n",
    "        x_train = pickle.load(file)\n",
    "\n",
    "    with open(y_train_pickle.path, \"rb\") as file:\n",
    "        y_train = pickle.load(file)\n",
    "\n",
    "    log_dir = f\"{log.path}/logs/fit/{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_units, input_dim=input_size))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(hidden_units))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(num_labels))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[tensorboard_callback],\n",
    "    )\n",
    "\n",
    "    model.save(model_artifact.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f024658d-6560-4aca-9608-396c6428b10b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_148/3553172685.py:1: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  @dsl.component(\n"
     ]
    }
   ],
   "source": [
    "@dsl.component(\n",
    "    base_image=BASE_IMAGE,\n",
    "    packages_to_install=[\"scikit-learn\"],\n",
    ")\n",
    "def evaluate(\n",
    "    model_artifact: Input[Model],\n",
    "    metrics: Output[ClassificationMetrics],\n",
    "    scalar_metrics: Output[Metrics],\n",
    "    x_test_pickle: Input[Dataset],\n",
    "    y_test_pickle: Input[Dataset],\n",
    "):\n",
    "    from keras.models import load_model\n",
    "    from keras.metrics import Precision\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "\n",
    "    model = load_model(model_artifact.path)\n",
    "\n",
    "    batch_size = 128\n",
    "\n",
    "    with open(x_test_pickle.path, \"rb\") as file:\n",
    "        x_test = pickle.load(file)\n",
    "\n",
    "    with open(y_test_pickle.path, \"rb\") as file:\n",
    "        y_test = pickle.load(file)\n",
    "\n",
    "    predictions = model.predict(x_test, batch_size=batch_size)\n",
    "    predictions = (predictions >= 0.5).astype(int)\n",
    "\n",
    "    metrics.log_confusion_matrix(\n",
    "        [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"],\n",
    "        confusion_matrix(\n",
    "            y_test.argmax(axis=1), predictions.argmax(axis=1)\n",
    "        ).tolist(),  # .tolist() to convert np array to list.\n",
    "    )\n",
    "    m = Precision()\n",
    "    m.update_state(y_test, predictions)\n",
    "    loss, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "    scalar_metrics.log_metric(\"accuracy\", acc)\n",
    "    scalar_metrics.log_metric(\"loss\", loss)\n",
    "    scalar_metrics.log_metric(\"precision\", m.result().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46696c0c-6cfc-4ed8-b6eb-59a832aa89b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/3898f581-f106-4bae-9f91-5aa4e39b4b20\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/9170bbd4-63c8-44bd-b4ff-1b00c4639f72\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=9170bbd4-63c8-44bd-b4ff-1b00c4639f72)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"mnist_pipeline\",\n",
    ")\n",
    "def mnist_pipeline(epochs: int):\n",
    "    data = (\n",
    "        load_data()\n",
    "        .set_memory_limit(\"4G\")\n",
    "        .set_memory_request(\"4G\")\n",
    "        .set_cpu_limit(\"2\")\n",
    "        .set_cpu_request(\"2\")\n",
    "    )\n",
    "    preprocess = (\n",
    "        preprocess_data(\n",
    "            x_train_pickle=data.outputs[\"x_train_pickle\"],\n",
    "            y_train_pickle=data.outputs[\"y_train_pickle\"],\n",
    "            x_test_pickle=data.outputs[\"x_test_pickle\"],\n",
    "            y_test_pickle=data.outputs[\"y_test_pickle\"],\n",
    "        )\n",
    "        .set_memory_limit(\"4G\")\n",
    "        .set_memory_request(\"4G\")\n",
    "        .set_cpu_limit(\"1\")\n",
    "        .set_cpu_request(\"1\")\n",
    "    )\n",
    "    preprocess.after(data)\n",
    "    model = (\n",
    "        train(\n",
    "            input_size=preprocess.outputs[\"input_size\"],\n",
    "            num_labels=preprocess.outputs[\"num_labels\"],\n",
    "            epochs=epochs,\n",
    "            x_train_pickle=preprocess.outputs[\"x_train_prep\"],\n",
    "            y_train_pickle=preprocess.outputs[\"y_train_prep\"],\n",
    "        )\n",
    "        .set_memory_limit(\"6G\")\n",
    "        .set_memory_request(\"6G\")\n",
    "        .set_cpu_limit(\"1\")\n",
    "        .set_cpu_request(\"1\")\n",
    "    )\n",
    "    model.after(preprocess)\n",
    "    evaluation = (\n",
    "        evaluate(\n",
    "            model_artifact=model.outputs[\"model_artifact\"],\n",
    "            x_test_pickle=preprocess.outputs[\"x_test_prep\"],\n",
    "            y_test_pickle=preprocess.outputs[\"y_test_prep\"],\n",
    "        )\n",
    "        .set_memory_limit(\"4G\")\n",
    "        .set_memory_request(\"4G\")\n",
    "        .set_cpu_limit(\"1\")\n",
    "        .set_cpu_request(\"1\")\n",
    "    )\n",
    "    evaluation.after(model)\n",
    "\n",
    "\n",
    "client = kfp.Client()\n",
    "client.create_run_from_pipeline_func(\n",
    "    mnist_pipeline,\n",
    "    arguments={\"epochs\": 30},\n",
    "    experiment_name=\"mnist_pipeline\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b706c9b-8c1b-4db5-a0ff-6c2d48d528cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
